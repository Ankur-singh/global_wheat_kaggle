cp_path: saved/weights
logs_path: saved/logs

trainer:
    gpus: 1
    accumulate_grad_batches: 8
    gradient_clip_val: 0.5
    max_epochs: &e 20
    fast_dev_run: False
    auto_lr_find: False

data:
    train_folds: train_folds.csv
    train_ext: train_ext.csv
    path: data
    fold: 0
    img_sz: 640
    batch_size: 2
    num_workers: 2

# # Best scheduler configuration so far in almost all the testing
# scheduler:
#     class_name: torch.optim.lr_scheduler.ReduceLROnPlateau
#     params:
#         mode: min
#         factor: 0.5
#         patience: 1
#         verbose: False
#         threshold: 0.0001
#         threshold_mode: abs
#         cooldown: 0 
#         min_lr: 1e-8
#         eps: 1e-08
#     step: epoch
#     monitor: val_loss  

# super fast convergence
scheduler:
    class_name: torch.optim.lr_scheduler.OneCycleLR
    params:
        pct_start: 0.3
        epochs: *e
        max_lr: 0.001
    steps_per_epoch: True
    step: step
    monitor: val_loss 
    
optimizer:
    class_name: torch.optim.Adam
    params:
        weight_decay: 0.001