cp_path: saved/weights/

trainer:
    gpus: 1
    limit_train_batches: 0.05 
    limit_val_batches: 0.5
    accumulate_grad_batches: 8
    gradient_clip_val: 0.5
    max_epochs: 10
    fast_dev_run: True
    auto_lr_find: False

data:
    train_folds: train_folds.csv
    train_ext: train_ext.csv
    path: data
    fold: 0
    img_sz: 640
    batch_size: 1
    num_workers: 2

scheduler:
    class_name: torch.optim.lr_scheduler.ReduceLROnPlateau
    params:
        factor: 0.1
        patience: 1
    step: epoch
    monitor: val_loss  

optimizer:
    class_name: torch.optim.Adam
    params:
        weight_decay: 0.001
        lr: 0.001
